{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb897df1",
   "metadata": {},
   "source": [
    "# 5.1. Layers and Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69afcfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0308,  0.2777, -0.3469,  0.0461,  0.0038, -0.0363, -0.0876,  0.1536,\n",
       "          0.0184,  0.1103],\n",
       "        [ 0.0115,  0.1792, -0.1553, -0.0465, -0.0214, -0.1112, -0.0146,  0.0237,\n",
       "          0.0601,  0.0244]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d645e65",
   "metadata": {},
   "source": [
    "## 5.1.1. A Custom Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb379d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # Declare a layer with model parameters. Here, we declare two fully\n",
    "    # connected layers\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the `MLP` parent class `Module` to perform\n",
    "        # the necessary initialization. In this way, other function arguments\n",
    "        # can also be specified during class instantiation, such as the model\n",
    "        # parameters, `params` (to be described later)\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # Hidden layer\n",
    "        self.out = nn.Linear(256, 10)  # Output layer\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input `X`\n",
    "    def forward(self, X):\n",
    "        # Note here we use the funtional version of ReLU defined in the\n",
    "        # nn.functional module.\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6088c178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1738,  0.1398, -0.0119, -0.0297, -0.0875, -0.0662, -0.0092,  0.0929,\n",
       "         -0.2273, -0.0150],\n",
       "        [ 0.1450, -0.0398, -0.0318, -0.0138, -0.0962, -0.0406, -0.0115,  0.1156,\n",
       "         -0.0477, -0.0141]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ff156",
   "metadata": {},
   "source": [
    "## 5.1.2. The Sequential Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d2a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # Here, `module` is an instance of a `Module` subclass. We save it\n",
    "            # in the member variable `_modules` of the `Module` class, and its\n",
    "            # type is OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict guarantees that members will be traversed in the order\n",
    "        # they were added\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b16437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1538,  0.0656,  0.0823, -0.1595,  0.2318, -0.2962, -0.1597, -0.1017,\n",
       "          0.0809, -0.1212],\n",
       "        [ 0.0398,  0.0494,  0.0740, -0.2106,  0.1352, -0.3015, -0.0705, -0.0108,\n",
       "          0.0888, -0.0439]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d149c50",
   "metadata": {},
   "source": [
    "## 5.1.3. Executing Code in the Forward Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d12ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # Use the created constant parameters, as well as the `relu` and `mm`\n",
    "        # functions\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # Reuse the fully-connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully-connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69e1961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2846, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3300f6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2296, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b28031",
   "metadata": {},
   "source": [
    "## 5.1.4. Efficiency\n",
    "\n",
    "The avid reader might start to worry about the efficiency of some of these operations. After all, we have lots of dictionary lookups, code execution, and lots of other Pythonic things taking place in what is supposed to be a high-performance deep learning library. The problems of Pythonâ€™s global interpreter lock are well known. In the context of deep learning, we may worry that our extremely fast GPU(s) might have to wait until a puny CPU runs Python code before it gets another job to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b2fc9",
   "metadata": {},
   "source": [
    "## 5.1.5. Summary\n",
    "\n",
    "Layers are blocks.\n",
    "\n",
    "Many layers can comprise a block.\n",
    "\n",
    "Many blocks can comprise a block.\n",
    "\n",
    "A block can contain code.\n",
    "\n",
    "Blocks take care of lots of housekeeping, including parameter initialization and backpropagation.\n",
    "\n",
    "Sequential concatenations of layers and blocks are handled by the Sequential block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
