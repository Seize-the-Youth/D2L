{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9718cdbe",
   "metadata": {},
   "source": [
    "# 5.6. GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bafc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f862207",
   "metadata": {},
   "source": [
    "## 5.6.1. Computing Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712107b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b0a2c",
   "metadata": {},
   "source": [
    "## 5.6.2. Tensors and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e69fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10c074",
   "metadata": {},
   "source": [
    "### 5.6.2.1. Storage on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3cd7fa",
   "metadata": {},
   "source": [
    "### 5.6.2.2. Copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f08a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X.cuda(1)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d25761",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.cuda(1) is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90afda",
   "metadata": {},
   "source": [
    "### 5.6.2.3. Side Notes\n",
    "\n",
    "People use GPUs to do machine learning because they expect them to be fast. But transferring variables between devices is slow. So we want you to be 100% certain that you want to do something slow before we let you do it. If the deep learning framework just did the copy automatically without crashing then you might not realize that you had written some slow code.\n",
    "\n",
    "Also, transferring data between devices (CPU, GPUs, and other machines) is something that is much slower than computation. It also makes parallelization a lot more difficult, since we have to wait for data to be sent (or rather to be received) before we can proceed with more operations. This is why copy operations should be taken with great care. As a rule of thumb, many small operations are much worse than one big operation. Moreover, several operations at a time are much better than many single operations interspersed in the code unless you know what you are doing. This is the case since such operations can block if one device has to wait for the other before it can do something else. It is a bit like ordering your coffee in a queue rather than pre-ordering it by phone and finding out that it is ready when you are.\n",
    "\n",
    "Last, when we print tensors or convert tensors to the NumPy format, if the data is not in the main memory, the framework will copy it to the main memory first, resulting in additional transmission overhead. Even worse, it is now subject to the dreaded global interpreter lock that makes everything wait for Python to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0266cd7",
   "metadata": {},
   "source": [
    "## 5.6.3. Neural Networks and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6763919",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bcd16",
   "metadata": {},
   "source": [
    "## 5.6.4. Summary\n",
    "\n",
    "We can specify devices for storage and calculation, such as the CPU or GPU. By default, data are created in the main memory and then use the CPU for calculations.\n",
    "\n",
    "The deep learning framework requires all input data for calculation to be on the same device, be it CPU or the same GPU.\n",
    "\n",
    "You can lose significant performance by moving data without care. A typical mistake is as follows: computing the loss for every minibatch on the GPU and reporting it back to the user on the command line (or logging it in a NumPy ndarray) will trigger a global interpreter lock which stalls all GPUs. It is much better to allocate memory for logging inside the GPU and only move larger logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
