{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d33230",
   "metadata": {},
   "source": [
    "# 6.2. Convolutions for Images\n",
    "\n",
    "## 6.2.1. The Cross-Correlation Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203135fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622355c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f4aba",
   "metadata": {},
   "source": [
    "## 6.2.2. Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8660e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064deec2",
   "metadata": {},
   "source": [
    "## 6.2.3. Object Edge Detection in Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440669f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bde9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2674c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c088b82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e9e6c",
   "metadata": {},
   "source": [
    "## 6.2.4. Learning a Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97ebe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 11.444\n",
      "epoch 4, loss 2.132\n",
      "epoch 6, loss 0.444\n",
      "epoch 8, loss 0.110\n",
      "epoch 10, loss 0.033\n"
     ]
    }
   ],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example, channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # Learning rate\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # Update the kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2657ecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9669, -0.9993]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72718db",
   "metadata": {},
   "source": [
    "## 6.2.5. Cross-Correlation and Convolution\n",
    "\n",
    "Recall our observation from Section 6.1 of the correspondence between the cross-correlation and convolution operations. Here let us continue to consider two-dimensional convolutional layers. What if such layers perform strict convolution operations as defined in (6.1.6) instead of cross-correlations? In order to obtain the output of the strict convolution operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the cross-correlation operation with the input tensor.\n",
    "\n",
    "It is noteworthy that since kernels are learned from data in deep learning, the outputs of convolutional layers remain unaffected no matter such layers perform either the strict convolution operations or the cross-correlation operations.\n",
    "\n",
    "To illustrate this, suppose that a convolutional layer performs cross-correlation and learns the kernel in Fig. 6.2.1, which is denoted as the matrix  K  here. Assuming that other conditions remain unchanged, when this layer performs strict convolution instead, the learned kernel  K′  will be the same as  K  after  K′  is flipped both horizontally and vertically. That is to say, when the convolutional layer performs strict convolution for the input in Fig. 6.2.1 and  K′ , the same output in Fig. 6.2.1 (cross-correlation of the input and  K ) will be obtained.\n",
    "\n",
    "In keeping with standard terminology with deep learning literature, we will continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different. Besides, we use the term element to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc959b",
   "metadata": {},
   "source": [
    "## 6.2.6. Feature Map and Receptive Field\n",
    "\n",
    "As described in Section 6.1.4.1, the convolutional layer output in Fig. 6.2.1 is sometimes called a feature map, as it can be regarded as the learned representations (features) in the spatial dimensions (e.g., width and height) to the subsequent layer. In CNNs, for any element  x  of some layer, its receptive field refers to all the elements (from all the previous layers) that may affect the calculation of  x  during the forward propagation. Note that the receptive field may be larger than the actual size of the input.\n",
    "\n",
    "Let us continue to use Fig. 6.2.1 to explain the receptive field. Given the  2×2  convolution kernel, the receptive field of the shaded output element (of value  19 ) is the four elements in the shaded portion of the input. Now let us denote the  2×2  output as  Y  and consider a deeper CNN with an additional  2×2  convolutional layer that takes  Y  as its input, outputting a single element  z . In this case, the receptive field of  z  on  Y  includes all the four elements of  Y , while the receptive field on the input includes all the nine input elements. Thus, when any element in a feature map needs a larger receptive field to detect input features over a broader area, we can build a deeper network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c232edf",
   "metadata": {},
   "source": [
    "## 6.2.7. Summary\n",
    "\n",
    "The core computation of a two-dimensional convolutional layer is a two-dimensional cross-correlation operation. In its simplest form, this performs a cross-correlation operation on the two-dimensional input data and the kernel, and then adds a bias.\n",
    "\n",
    "We can design a kernel to detect edges in images.\n",
    "\n",
    "We can learn the kernel’s parameters from data.\n",
    "\n",
    "With kernels learned from data, the outputs of convolutional layers remain unaffected regardless of such layers’ performed operations (either strict convolution or cross-correlation).\n",
    "\n",
    "When any element in a feature map needs a larger receptive field to detect broader features on the input, a deeper network can be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
