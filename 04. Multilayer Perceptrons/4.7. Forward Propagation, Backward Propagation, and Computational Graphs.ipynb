{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db206e20",
   "metadata": {},
   "source": [
    "# 4.7. Forward Propagation, Backward Propagation, and Computational Graphs\n",
    "\n",
    "So far, we have trained our models with minibatch stochastic gradient descent. However, when we implemented the algorithm, we only worried about the calculations involved in forward propagation through the model. When it came time to calculate the gradients, we just invoked the backpropagation function provided by the deep learning framework.\n",
    "\n",
    "The automatic calculation of gradients (automatic differentiation) profoundly simplifies the implementation of deep learning algorithms. Before automatic differentiation, even small changes to complicated models required recalculating complicated derivatives by hand. Surprisingly often, academic papers had to allocate numerous pages to deriving update rules. While we must continue to rely on automatic differentiation so we can focus on the interesting parts, you ought to know how these gradients are calculated under the hood if you want to go beyond a shallow understanding of deep learning.\n",
    "\n",
    "In this section, we take a deep dive into the details of backward propagation (more commonly called backpropagation). To convey some insight for both the techniques and their implementations, we rely on some basic mathematics and computational graphs. To start, we focus our exposition on a one-hidden-layer MLP with weight decay ( L2  regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f783b8f",
   "metadata": {},
   "source": [
    "## 4.7.1. Forward Propagation\n",
    "\n",
    "Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer. We now work step-by-step through the mechanics of a neural network with one hidden layer. This may seem tedious but in the eternal words of funk virtuoso James Brown, you must “pay the cost to be the boss”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b1a8b",
   "metadata": {},
   "source": [
    "## 4.7.2. Computational Graph of Forward Propagation\n",
    "\n",
    "Plotting computational graphs helps us visualize the dependencies of operators and variables within the calculation. Fig. 4.7.1 contains the graph associated with the simple network described above, where squares denote variables and circles denote operators. The lower-left corner signifies the input and the upper-right corner is the output. Notice that the directions of the arrows (which illustrate data flow) are primarily rightward and upward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ddac7",
   "metadata": {},
   "source": [
    "## 4.7.3. Backpropagation\n",
    "\n",
    "Backpropagation refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b33382",
   "metadata": {},
   "source": [
    "## 4.7.4. Training Neural Networks\n",
    "\n",
    "When training neural networks, forward and backward propagation depend on each other. In particular, for forward propagation, we traverse the computational graph in the direction of dependencies and compute all the variables on its path. These are then used for backpropagation where the compute order on the graph is reversed.\n",
    "\n",
    "Take the aforementioned simple network as an example to illustrate. On one hand, computing the regularization term (4.7.5) during forward propagation depends on the current values of model parameters  W(1)  and  W(2) . They are given by the optimization algorithm according to backpropagation in the latest iteration. On the other hand, the gradient calculation for the parameter (4.7.11) during backpropagation depends on the current value of the hidden variable  h , which is given by forward propagation.\n",
    "\n",
    "Therefore when training neural networks, after model parameters are initialized, we alternate forward propagation with backpropagation, updating model parameters using gradients given by backpropagation. Note that backpropagation reuses the stored intermediate values from forward propagation to avoid duplicate calculations. One of the consequences is that we need to retain the intermediate values until backpropagation is complete. This is also one of the reasons why training requires significantly more memory than plain prediction. Besides, the size of such intermediate values is roughly proportional to the number of network layers and the batch size. Thus, training deeper networks using larger batch sizes more easily leads to out of memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889981c5",
   "metadata": {},
   "source": [
    "## 4.7.5. Summary\n",
    "\n",
    "Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer.\n",
    "\n",
    "Backpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order.\n",
    "\n",
    "When training deep learning models, forward propagation and back propagation are interdependent.\n",
    "\n",
    "Training requires significantly more memory than prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
