{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de74c05",
   "metadata": {},
   "source": [
    "# 7.7. Densely Connected Networks (DenseNet)\n",
    "ResNet significantly changed the view of how to parametrize the functions in deep networks. DenseNet (dense convolutional network) is to some extent the logical extension of this [Huang et al., 2017]. To understand how to arrive at it, let us take a small detour to mathematics.\n",
    "\n",
    "## 7.7.1. From ResNet to DenseNet\n",
    "Recall the Taylor expansion for functions. For the point  x=0  it can be written as\n",
    "\n",
    "f(x)=f(0)+f′(0)x+f′′(0)2!x2+f′′′(0)3!x3+….\n",
    " \n",
    "The key point is that it decomposes a function into increasingly higher order terms. In a similar vein, ResNet decomposes functions into\n",
    "\n",
    "f(x)=x+g(x).\n",
    " \n",
    "That is, ResNet decomposes  f  into a simple linear term and a more complex nonlinear one. What if we want to capture (not necessarily add) information beyond two terms? One solution was DenseNet [Huang et al., 2017].\n",
    "\n",
    "As shown in Fig. 7.7.1, the key difference between ResNet and DenseNet is that in the latter case outputs are concatenated (denoted by  [,] ) rather than added. As a result, we perform a mapping from  x  to its values after applying an increasingly complex sequence of functions:\n",
    "\n",
    "x→[x,f1(x),f2([x,f1(x)]),f3([x,f1(x),f2([x,f1(x)])]),…].\n",
    " \n",
    "In the end, all these functions are combined in MLP to reduce the number of features again. In terms of implementation this is quite simple: rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The last layer of such a chain is densely connected to all previous layers. The dense connections are shown in Fig. 7.7.2.\n",
    "\n",
    "The main components that compose a DenseNet are dense blocks and transition layers. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c34305",
   "metadata": {},
   "source": [
    "## 7.7.2. Dense Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d061536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6a6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(\n",
    "                num_channels * i + input_channels, num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel\n",
    "            # dimension\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3653ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06623f0f",
   "metadata": {},
   "source": [
    "## 7.7.3. Transition Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa486452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf646243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1978d",
   "metadata": {},
   "source": [
    "## 7.7.4. DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f42338",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b1d2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `num_channels`: the current number of channels\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "blks = []\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # This is the number of output channels in the previous dense block\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that halves the number of channels is added between\n",
    "    # the dense blocks\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        blks.append(transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28e8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    b1, *blks,\n",
    "    nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_channels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6cf2c",
   "metadata": {},
   "source": [
    "## 7.7.5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76628d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61296f",
   "metadata": {},
   "source": [
    "## 7.7.6. Summary\n",
    "In terms of cross-layer connections, unlike ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension.\n",
    "\n",
    "The main components that compose DenseNet are dense blocks and transition layers.\n",
    "\n",
    "We need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
